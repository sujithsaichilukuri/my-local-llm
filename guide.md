

The actual guide starts here!

> [!NOTE]
> No specialized hardware is required.

This guide uses [Podman](https://podman.io/) and the terminal/console to get everything up and running. Some familiarity with using the terminal/console on your system is required. All the commands commands *should* work on the following operating systems: Linux, Windows\*\*, and MacOS\*\*.

> [!NOTE]
> \*\* If you're using Windows or MacOS, then [Podman Desktop](https://podman-desktop.io/) is required.

A deep understanding of how containers work or how podman works is *not* required but will be helpful when expanding on what you've learned from this guide.

## Ollama

We'll be using the [ollama image from Docker.io](https://hub.docker.com/r/ollama/ollama). The image has the Ollama CLI already configure, which we'll be using to download and run models.

The first thing that we'll need to do is create a volume to store our downloaded models.

```bash
podman volume create ollama
```

After create the volume, the container can be created and started.

```bash
podman run -d -v ollama:/root/.ollama --name ollama docker.io/ollama/ollama
```

Now that the container is running, let's verify that we can execute the Ollama CLI by displaying the version number.

```bash
podman exec ollama ollama --version
```

Executing that command should output something like below:

```
ollama version is 0.4.3
```

Now that we are able to run the CLI, let's download out first model. We'll be downloading the [smollm2:135m-instruct-q6_K](https://ollama.com/library/smollm2:135m-instruct-q6_K) because it's a very small model which means it requires very little resources. To download a model, we can use the CLI's `pull` command.

> [!NOTE]
> To download any model using ollama, the model name and tag needs to be provided. The format is, `model_name:tag_name`. Eg. `smollm2:135m-instruct-q6_K` the model name is `smollm2` and the tag is `135m-instruct-q6_K`.

```bash
podman exec ollama ollama pull smollm2:135m-instruct-q6_K
```

Once it down downloading, we can run the model using the CLI's `run` command.

```bash
podman exec -it ollama ollama run smollm2:135m-instruct-q6_K
```

Let's ask the model something!

```
Why is the sky blue?
```

We can close the chat by typing in `/bye` or using the keyboard short cut control + d.

## Running Open WebUI

The Ollama CLI lacks a fair bit of quality of life features which is where Open WebUI comes in. It's a web application that can be used as a wrapper around Ollama.

If you still have the Ollama container running from the first part of this guide, you'll need to stop and remove that container so that we can re-create.

> [!NOTE]
> You can use the `podman container ls` command to see if the Ollama container is running

```bash
podman container stop ollama
podman container rm ollama
```

Once the Ollama container is stopped and removed, we'll need to create a network so that the Open WebUI container and the new Ollama container can communicate with each other.

```bash
podman network create ollama
```

Once the network is created, we will use a slightly different command to re-create the Ollama container.

```bash
podman run -d --network ollama -v ollama:/root/.ollama -p 11434:11434 --name ollama docker.io/ollama/ollama
```

Before we can start the Open WebUI container, we need to create another volume so that the Open WebUI container can store setting and user information.

```bash
podman volume create open-webui
```

Once that is complete, we can now start the Open WebUI container.

```bash
podman run -d -p 3000:8080 --network ollama -e OLLAMA_BASE_URL=http://ollama:11434 -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main
```

Depending on the machine you're runnig these command, it might take a while for the container is fully initialize. We can see the logs from the container using the command below.

```bash
podman logs -f open-webui
```

Once the something like: `INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)` appears in the logs, the web server is running. Press control + c to stop following the logs.

Open up your browser and navigate to [`localhost:3000`](http://localhost:3000) (*not* port `8080`!). You should be able to see the start up screen. Complete the set up process which includes creating an admin account.

Once complete you'll be able to login! To chat with the model that we download earlier, click on the dropdown near the top left of the screen. By default it should say, "Area Model". Select the model that we download early, `smollm2:135m-instruct-q6_K`.

Ask it the same question we asked earlier, "Why is the sky blue?".

> [!NOTE]
> One thing to note, the containers will not start automatically when after a system reboot. For that, you'll need to use something like `systemd`, `quadlet`, `compose`, or any other method. There are a few examples in the [`configuration-examples`](configuration-examples/) directory to help get you get started.

For a complete list of [features](https://docs.openwebui.com/features/), [pipelines](https://docs.openwebui.com/pipelines/), [tutorials](https://docs.openwebui.com/category/-tutorials), etc, see the [official Open WebUI documentation](https://docs.openwebui.com/).
